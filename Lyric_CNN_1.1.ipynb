{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math as m\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import gensim \n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER VARIABLES\n",
    "#### Uncomment on server\n",
    "# device = device_lib.list_local_devices()\n",
    "# print(device)\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# backend.set_session(sess)\n",
    "\n",
    "# NO GPU so must downsize\n",
    "CPU=True\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "VALIDATION_SPLIT = 0.33\n",
    "TEST_SPLIT = 0.2\n",
    "learning_rate = .001\n",
    "max_grad_norm = 1.\n",
    "DROPOUT = 0.5\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# PATH CONSTANTS\n",
    "PICKLE_ROOT = 'data/lyrics/'\n",
    "PICKLE_INPUT = 'CNN_input.pickle' \n",
    "\n",
    "EMBEDDING_PATH = 'data/glove_embeddings/'\n",
    "EMBEDDING_FILE = 'glove.6B.'+str(EMBEDDING_DIM)+'d.txt'\n",
    "\n",
    "MODEL_DIR = 'saved_models/'\n",
    "MODEL_SAVE_FILE = 'cnn_model_1.1_'+str(epochs)+'.json'\n",
    "MODEL_SAVE_WEIGHTS_FILE = 'cnn_model_1.1_'+str(epochs)+'.h5'\n",
    "BEST_WEIGHTS_FILE = 'best_weights'+str(epochs)+'.hdf5'\n",
    "\n",
    "GRAPHS_DIR = 'graphs_out/'\n",
    "\n",
    "DOC2VEC_PATH = MODEL_DIR + 'doc2vec/'\n",
    "DOC2VEC_FILE = 'd2v.model'\n",
    "\n",
    "# Default values - changed later\n",
    "MAX_SONG_LENGTH = 2500\n",
    "MAX_UNIQUE_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding\n",
      "finished loading embedding\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "# Elmo could improve the word embeddings - need more research\n",
    "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "print('loading embedding')\n",
    "try:\n",
    "    if not os.path.exists(EMBEDDING_PATH+EMBEDDING_FILE):\n",
    "        print('Embeddings not found, downloading now')\n",
    "        try:\n",
    "            print(os.system('pwd'))\n",
    "            os.system(' cd ' + DATA_PATH)\n",
    "            os.system(' mkdir ' + EMBEDDING_DIR)\n",
    "            os.system(' cd ' + EMBEDDING_DIR)\n",
    "            os.system(' wget http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            os.system(' unzip glove.6B.zip')\n",
    "            os.system(' cd ../..')\n",
    "        except:\n",
    "            print('not optimized for this operating system.')\n",
    "            print('please download: ')\n",
    "            print('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            print('Note: this may take a while')\n",
    "            sys.exit()\n",
    "except:\n",
    "    print('Do you have the word embeddings?')\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(EMBEDDING_PATH+EMBEDDING_FILE, encoding='utf-8') as emb_f:\n",
    "    for line in emb_f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "print('finished loading embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickles\n",
      "number of songs: 22500\n",
      "number of genres: 5\n",
      "number of lyrics: 22500\n",
      "number of unique words: 54404\n",
      "longest song: 1300\n",
      "finished loading pickles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('loading pickles')\n",
    "pickle_data = pickle.load( open(PICKLE_ROOT + PICKLE_INPUT , \"rb\" ))\n",
    "lyrics = pickle_data['lyrics']\n",
    "lyrics_labels = pickle_data['lyrics_labels']\n",
    "unique_words_set = pickle_data['unique_words_set']\n",
    "genre_index = pickle_data['genre_index']\n",
    "MAX_SONG_LENGTH = round(pickle_data['longest_song'],-2)\n",
    "print('number of songs: %d' %(len(lyrics)))\n",
    "print('number of genres: %d' %(len(genre_index)))\n",
    "print('number of lyrics: %d' %(len(lyrics_labels)))\n",
    "print('number of unique words: %d' %(len(unique_words_set)))\n",
    "print('longest song: %d' %(MAX_SONG_LENGTH))\n",
    "print('finished loading pickles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n",
      "Unique words tokens 54391\n",
      "finished tokenizing\n"
     ]
    }
   ],
   "source": [
    "# MAX_UNIQUE_WORDS = len(unique_words_set)\n",
    "# MAX_SONG_LENGTH = 1000\n",
    "# data preparing\n",
    "print('tokenizing')\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "sequences = tokenizer.texts_to_sequences(lyrics)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Unique words tokens %d' % (len(word_index)))\n",
    "\n",
    "data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SONG_LENGTH,padding='post')\n",
    "labels = keras.utils.to_categorical(np.asarray(lyrics_labels))\n",
    "\n",
    "print('finished tokenizing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model(nn_model,filename,weights_filename):\n",
    "    # serialize model to JSON\n",
    "    model_json = nn_model.to_json()\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    nn_model.save_weights(weights_filename)\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(filename,weights_filename):\n",
    "    # load json and create model\n",
    "    json_file = open(filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_filename)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_data(ind):\n",
    "    filename = 'recent_testdata_'+str(epochs)+'.pickle'\n",
    "    data_ind = {}\n",
    "    data_ind['indices'] = ind\n",
    "    pickle.dump( data_ind, open(PICKLE_ROOT+filename, \"wb\" ) )\n",
    "    print('saved test data to %s%s' %(PICKLE_ROOT,filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved test data to data/lyrics/recent_testdata_1.pickle\n",
      "data tensor: (22500, 1300)\n",
      "test tensor: (450, 1300)\n",
      "train tensor: (1800, 1300)\n",
      "valid splits:  False\n",
      "label tensor: (22500, 5)\n",
      "test tensor: (450, 5)\n",
      "train tensor: (1800, 5)\n",
      "valid splits:  False\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "save_test_data(indices)\n",
    "\n",
    "t_data = data[:int(data.shape[0]*.1)]\n",
    "t_labels = labels[:int(data.shape[0]*.1)]\n",
    "\n",
    "if not CPU:\n",
    "    num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "    x_test = data[:num_test_samples]\n",
    "    y_test = labels[:num_test_samples]\n",
    "    x_train = data[num_test_samples:]\n",
    "    y_train = labels[num_test_samples:]\n",
    "    \n",
    "else:\n",
    "    num_test_samples = int(TEST_SPLIT * t_data.shape[0])\n",
    "\n",
    "    x_test = t_data[:num_test_samples]\n",
    "    y_test = t_labels[:num_test_samples]\n",
    "    x_train = t_data[num_test_samples:]\n",
    "    y_train = t_labels[num_test_samples:]\n",
    "\n",
    "print('data tensor:', data.shape)\n",
    "print('test tensor:', x_test.shape)\n",
    "print('train tensor:', x_train.shape)\n",
    "print('valid splits: ', x_test.shape[0]+x_train.shape[0] == data.shape[0])\n",
    "print('label tensor:', labels.shape)\n",
    "print('test tensor:', y_test.shape)\n",
    "print('train tensor:', y_train.shape)\n",
    "print('valid splits: ', y_test.shape[0]+y_train.shape[0] == data.shape[0])\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "unique_words_count = min(MAX_UNIQUE_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((unique_words_count, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_UNIQUE_WORDS:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # potentially can improve if OOV words are handled differently        \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00566267 -0.01566155 -0.01421482  0.01551718  0.01920173 -0.00205161\n",
      "  0.01440403  0.02459881 -0.00089787 -0.00277866 -0.00214458 -0.02301646\n",
      " -0.00134379  0.02197527 -0.00231293 -0.0036914   0.02286785 -0.00414305\n",
      " -0.01773595  0.00604648]\n"
     ]
    }
   ],
   "source": [
    "# loading doc2vec model\n",
    "# train_lyrics = np.array(lyrics)[indices]\n",
    "# print(train_lyrics[0])\n",
    "# def read_corpus(_data, tokens_only=False):\n",
    "#     i = 0\n",
    "#     for key,line in _data.items():\n",
    "#         if tokens_only:\n",
    "#             yield gensim.utils.simple_preprocess(line)\n",
    "#         else:\n",
    "#             # For training data, add tags\n",
    "#             yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "#         i+=1\n",
    "        \n",
    "# train_corpus = list(read_corpus(train_lyrics))\n",
    "# model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "# model.build_vocab(train_corpus)\n",
    "# %time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# def lyric2vec():\n",
    "    \n",
    "    \n",
    "\n",
    "# d2v_model = Doc2Vec.load(DOC2VEC_PATH + DOC2VEC_FILE)\n",
    "# print(d2v_model.infer_vector(train_lyrics[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2dconv_model():\n",
    "    embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SONG_LENGTH,\n",
    "                                trainable=True)\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SONG_LENGTH,))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "#     print(embedded_sequences.shape)\n",
    "    # add first conv filter\n",
    "    embedded_sequences = Reshape((MAX_SONG_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n",
    "    x = Conv2D(100, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling2D((MAX_SONG_LENGTH - 10 + 1, 1))(x)\n",
    "    # add second conv filter.\n",
    "    y = Conv2D(100, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    y = MaxPooling2D((MAX_SONG_LENGTH - 8 + 1, 1))(y)\n",
    "    # add third conv filter.\n",
    "    z = Conv2D(100, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    z = MaxPooling2D((MAX_SONG_LENGTH - 6 + 1, 1))(z)\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x,y,z])\n",
    "\n",
    "#     alpha = Dropout(0.5)(alpha)\n",
    "    \n",
    "    # flatted the pooled features.\n",
    "    alpha = Flatten()(alpha)\n",
    "    \n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "    \n",
    "#   alpha = Dense(50, activation='relu')(alpha)\n",
    "    \n",
    "    # predictions\n",
    "    preds = Dense(len(genre_index), activation='softmax')(alpha)\n",
    "    # build model\n",
    "    model = Model(sequence_input, preds)\n",
    "#     adadelta = optimizers.Adam()\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "WARNING:tensorflow:From /Users/Joe/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/Joe/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1300)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1300, 100)    2000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1300, 100, 1) 0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1296, 1, 100) 50100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1297, 1, 100) 40100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1298, 1, 100) 30100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 100)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 1, 300)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 300)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            1505        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,121,805\n",
      "Trainable params: 2,121,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model')\n",
    "# opt = tf.keras.optimizers.Adam(lr=learning_rate, clipnorm=max_grad_norm)\n",
    "# model = create_basic_cnn_model()\n",
    "# model = create_complex_cnn_model(False)\n",
    "checkpointer = ModelCheckpoint(filepath=BEST_WEIGHTS_FILE, \n",
    "                               monitor = 'val_acc',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "model = create_2dconv_model()\n",
    "# model = create_conv_lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Train on 1205 samples, validate on 595 samples\n",
      "WARNING:tensorflow:From /Users/Joe/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "1152/1205 [===========================>..] - ETA: 3s - loss: 1.5599 - acc: 0.3012 \n",
      "Epoch 00001: val_acc improved from -inf to 0.46723, saving model to best_weights1.hdf5\n",
      "1205/1205 [==============================] - 84s 70ms/sample - loss: 1.5570 - acc: 0.3012 - val_loss: 1.4069 - val_acc: 0.4672\n",
      "Test loss: 1.3894029569625854\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "print('Training Model')\n",
    "model_details = model.fit(x_train, y_train,\n",
    "            batch_size=128,\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1,\n",
    "            validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "scores= model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1800, 1300)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "input_1:0 is both fed and fetched.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1dcddaf8b707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlayer_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3071\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3072\u001b[0m         session != self._session):\n\u001b[0;32m-> 3073\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m     \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m     \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: input_1:0 is both fed and fetched."
     ]
    }
   ],
   "source": [
    "# # layer_outputs = [layer.output for layer in model.layers]\n",
    "# print(x_train[0].shape)\n",
    "# print(type(x_train[0]))\n",
    "# print(x_train.shape)\n",
    "\n",
    "# inp = model.input                                           # input placeholder\n",
    "# outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "# functor = backend.function([inp, backend.learning_phase()], outputs )   # evaluation function\n",
    "\n",
    "# # Testing\n",
    "# layer_outs = functor([x_train[0], 1.])\n",
    "# print(layer_outs)\n",
    "\n",
    "\n",
    "\n",
    "# layer_outputs = [layer.output for layer in model.layers if layer.name == layer_name or layer_name is None][1:]\n",
    "# activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# activations = activation_model.predict(x_train[0].reshape(1,1300))\n",
    "\n",
    "# def display_activation(activations, col_size, row_size, act_index): \n",
    "#     activation = activations[act_index]\n",
    "#     print(activation)\n",
    "#     activation_index=0\n",
    "#     fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
    "#     for row in range(0,row_size):\n",
    "#         for col in range(0,col_size):\n",
    "#             ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "#             activation_index += 1\n",
    "\n",
    "# display_activation(activations, 8, 8, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, error):\n",
    "    cmap=plt.cm.Blues\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    print(classes)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "    plt.title('Average Recognition Accuracy: %.02f' %(error))\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def check_accuracy(model2,x_test,y_test):\n",
    "    print('%d,%d'%(len(x_test),len(y_test)))\n",
    "    y_pred = model2.predict(x_test,verbose=0)\n",
    "    matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    accuracy = np.sum(np.identity(len(genre_index))*matrix)/len(y_test)\n",
    "    print('Accuracy: %.2f' %(accuracy))\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(matrix,list(genre_index.keys()),accuracy)\n",
    "    plt.savefig(GRAPHS_DIR+'confusion_matrix_'+str(epochs)+'.png')\n",
    "    print('saved confusion matrix')\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'accuracy_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved accuracy')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'loss_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model,x_test,y_test)\n",
    "plot_data(model_details)\n",
    "save_model(model,MODEL_SAVE_FILE, MODEL_SAVE_WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## OLD MODELS ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_basic_cnn_model():\n",
    "#     embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "#                                 EMBEDDING_DIM,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 input_length=MAX_SONG_LENGTH,\n",
    "#                                 trainable=False)\n",
    "#     model = Sequential()\n",
    "#     model.add(embedding_layer)\n",
    "#     model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "#     model.add(keras.layers.GlobalMaxPooling1D())\n",
    "#     model.add(keras.layers.Dense(10, activation='relu'))\n",
    "#     model.add(keras.layers.Dense(len(genre_index), activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['acc'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_conv_lstm_model():\n",
    "#     embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "#                                 EMBEDDING_DIM,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 input_length=MAX_SONG_LENGTH,\n",
    "#                                 trainable=False)\n",
    "#     model_conv = Sequential()\n",
    "#     model_conv.add(embedding_layer)\n",
    "#     model_conv.add(Dropout(0.2))\n",
    "#     model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "#     model_conv.add(MaxPooling1D(pool_size=4))\n",
    "#     model_conv.add(LSTM(EMBEDDING_DIM))\n",
    "#     model_conv.add(Dense(len(genre_index), activation='softmax'))\n",
    "#     model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "#     return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_complex_cnn_model(embedding_trained):\n",
    "\n",
    "#     embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "#                                 EMBEDDING_DIM,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 input_length=MAX_SONG_LENGTH,\n",
    "#                                 trainable=embedding_trained)\n",
    "#     CONV1D_OUT = 256\n",
    "#     CONV1D_OUT2 = CONV1D_OUT//2\n",
    "#     CONV1D_OUT3 = CONV1D_OUT2//2\n",
    "#     OUT = 5\n",
    "#     MAX_POOLING_1 = 8\n",
    "#     MAX_POOLING_2 = 8\n",
    "#     MAX_POOLING_4 = 13 #when MAX_SONG_LENGTH = 1300\n",
    "#     # MAX_POOLING_3 = 47 #when MAX_SONG_LENGTH = 1300\n",
    "#     # MAX_POOLING_2 = 35 #when MAX_SONG_LENGTH = 1000\n",
    "#     # DROPOUT = 0.5\n",
    "\n",
    "#     # conv1d = single spatial convolution of 2d input (sequence of 1000 - 200 demention vectors)\n",
    "#     # maxpooling1d = randomly downsizes by pooling val\n",
    "#     # dropout = randomly zeros at dropout rate to avoid overfitting\n",
    "\n",
    "#     model2 = Sequential()\n",
    "#     model2.add(embedding_layer)\n",
    "\n",
    "#     model2.add(tf.keras.layers.Conv1D(CONV1D_OUT, OUT, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.MaxPooling1D(MAX_POOLING_1))\n",
    "#     model2.add(tf.keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "#     model2.add(tf.keras.layers.Conv1D(CONV1D_OUT2, OUT, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.MaxPooling1D(MAX_POOLING_2))\n",
    "#     model2.add(tf.keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "#     model2.add(tf.keras.layers.Conv1D(CONV1D_OUT2, OUT, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.MaxPooling1D(MAX_POOLING_3))\n",
    "#     model2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#     model2.add(tf.keras.layers.Dense(CONV1D_OUT3, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.Dense(len(genre_index), activation='softmax'))\n",
    "\n",
    "#     # opt = tf.keras.optimizers.RMSprop(lr=learning_rate, clipnorm=max_grad_norm)\n",
    "#     model2.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['acc'])\n",
    "#     return model2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
