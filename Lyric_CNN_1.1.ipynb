{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math as m\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import gensim \n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER VARIABLES\n",
    "#### Uncomment on server\n",
    "# device = device_lib.list_local_devices()\n",
    "# print(device)\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# backend.set_session(sess)\n",
    "\n",
    "# NO GPU so must downsize\n",
    "CPU=True\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "VALIDATION_SPLIT = 0.33\n",
    "TEST_SPLIT = 0.2\n",
    "learning_rate = .001\n",
    "max_grad_norm = 1.\n",
    "DROPOUT = 0.5\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# PATH CONSTANTS\n",
    "PICKLE_ROOT = 'data/lyrics/'\n",
    "PICKLE_INPUT = 'CNN_input.pickle' \n",
    "\n",
    "EMBEDDING_PATH = 'data/glove_embeddings/'\n",
    "EMBEDDING_FILE = 'glove.6B.'+str(EMBEDDING_DIM)+'d.txt'\n",
    "\n",
    "MODEL_DIR = 'saved_models/'\n",
    "MODEL_SAVE_FILE = 'cnn_model_1.1_'+str(epochs)+'.json'\n",
    "MODEL_SAVE_WEIGHTS_FILE = 'cnn_model_1.1_'+str(epochs)+'.h5'\n",
    "BEST_WEIGHTS_FILE = 'best_weights'+str(epochs)+'.hdf5'\n",
    "\n",
    "GRAPHS_DIR = 'graphs_out/'\n",
    "\n",
    "DOC2VEC_PATH = MODEL_DIR + 'doc2vec/'\n",
    "DOC2VEC_FILE = 'd2v.model'\n",
    "\n",
    "# Default values - changed later\n",
    "MAX_SONG_LENGTH = 2500\n",
    "MAX_UNIQUE_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding\n",
      "finished loading embedding\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "# Elmo could improve the word embeddings - need more research\n",
    "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "print('loading embedding')\n",
    "try:\n",
    "    if not os.path.exists(EMBEDDING_PATH+EMBEDDING_FILE):\n",
    "        print('Embeddings not found, downloading now')\n",
    "        try:\n",
    "            print(os.system('pwd'))\n",
    "            os.system(' cd ' + DATA_PATH)\n",
    "            os.system(' mkdir ' + EMBEDDING_DIR)\n",
    "            os.system(' cd ' + EMBEDDING_DIR)\n",
    "            os.system(' wget http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            os.system(' unzip glove.6B.zip')\n",
    "            os.system(' cd ../..')\n",
    "        except:\n",
    "            print('not optimized for this operating system.')\n",
    "            print('please download: ')\n",
    "            print('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            print('Note: this may take a while')\n",
    "            sys.exit()\n",
    "except:\n",
    "    print('Do you have the word embeddings?')\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(EMBEDDING_PATH+EMBEDDING_FILE, encoding='utf-8') as emb_f:\n",
    "    for line in emb_f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "print('finished loading embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickles\n",
      "number of songs: 22500\n",
      "number of genres: 5\n",
      "number of lyrics: 22500\n",
      "number of unique words: 54404\n",
      "longest song: 1300\n",
      "finished loading pickles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('loading pickles')\n",
    "pickle_data = pickle.load( open(PICKLE_ROOT + PICKLE_INPUT , \"rb\" ))\n",
    "lyrics = pickle_data['lyrics']\n",
    "lyrics_labels = pickle_data['lyrics_labels']\n",
    "unique_words_set = pickle_data['unique_words_set']\n",
    "genre_index = pickle_data['genre_index']\n",
    "MAX_SONG_LENGTH = round(pickle_data['longest_song'],-2)\n",
    "print('number of songs: %d' %(len(lyrics)))\n",
    "print('number of genres: %d' %(len(genre_index)))\n",
    "print('number of lyrics: %d' %(len(lyrics_labels)))\n",
    "print('number of unique words: %d' %(len(unique_words_set)))\n",
    "print('longest song: %d' %(MAX_SONG_LENGTH))\n",
    "print('finished loading pickles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n",
      "Unique words tokens 54391\n",
      "finished tokenizing\n"
     ]
    }
   ],
   "source": [
    "# MAX_UNIQUE_WORDS = len(unique_words_set)\n",
    "# MAX_SONG_LENGTH = 1000\n",
    "# data preparing\n",
    "print('tokenizing')\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "sequences = tokenizer.texts_to_sequences(lyrics)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Unique words tokens %d' % (len(word_index)))\n",
    "\n",
    "data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SONG_LENGTH,padding='post')\n",
    "labels = keras.utils.to_categorical(np.asarray(lyrics_labels))\n",
    "\n",
    "print('finished tokenizing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model(nn_model,filename,weights_filename):\n",
    "    # serialize model to JSON\n",
    "    model_json = nn_model.to_json()\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    nn_model.save_weights(weights_filename)\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(filename,weights_filename):\n",
    "    # load json and create model\n",
    "    json_file = open(filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_filename)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_data(ind):\n",
    "    filename = 'recent_testdata_'+str(epochs)+'.pickle'\n",
    "    data_ind = {}\n",
    "    data_ind['indices'] = ind\n",
    "    pickle.dump( data_ind, open(PICKLE_ROOT+filename, \"wb\" ) )\n",
    "    print('saved test data to %s%s' %(PICKLE_ROOT,filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved test data to data/lyrics/recent_testdata_20.pickle\n",
      "data tensor: (22500, 1300)\n",
      "test tensor: (450, 1300)\n",
      "train tensor: (1800, 1300)\n",
      "valid splits:  False\n",
      "label tensor: (22500, 5)\n",
      "test tensor: (450, 5)\n",
      "train tensor: (1800, 5)\n",
      "valid splits:  False\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "save_test_data(indices)\n",
    "\n",
    "t_data = data[:int(data.shape[0]*.1)]\n",
    "t_labels = labels[:int(data.shape[0]*.1)]\n",
    "\n",
    "if not CPU:\n",
    "    num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "    x_test = data[:num_test_samples]\n",
    "    y_test = labels[:num_test_samples]\n",
    "    x_train = data[num_test_samples:]\n",
    "    y_train = labels[num_test_samples:]\n",
    "    \n",
    "else:\n",
    "    num_test_samples = int(TEST_SPLIT * t_data.shape[0])\n",
    "\n",
    "    x_test = t_data[:num_test_samples]\n",
    "    y_test = t_labels[:num_test_samples]\n",
    "    x_train = t_data[num_test_samples:]\n",
    "    y_train = t_labels[num_test_samples:]\n",
    "\n",
    "print('data tensor:', data.shape)\n",
    "print('test tensor:', x_test.shape)\n",
    "print('train tensor:', x_train.shape)\n",
    "print('valid splits: ', x_test.shape[0]+x_train.shape[0] == data.shape[0])\n",
    "print('label tensor:', labels.shape)\n",
    "print('test tensor:', y_test.shape)\n",
    "print('train tensor:', y_train.shape)\n",
    "print('valid splits: ', y_test.shape[0]+y_train.shape[0] == data.shape[0])\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "unique_words_count = min(MAX_UNIQUE_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((unique_words_count, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_UNIQUE_WORDS:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # potentially can improve if OOV words are handled differently        \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0067826  -0.00073149  0.02123053  0.00317578 -0.00583018  0.01566315\n",
      "  0.01833521  0.0035891  -0.01507775  0.0080893   0.00065948  0.0109284\n",
      "  0.01483469 -0.01924025  0.0001595  -0.00149134 -0.00852708  0.01897708\n",
      "  0.01926945 -0.02103674]\n"
     ]
    }
   ],
   "source": [
    "# loading doc2vec model\n",
    "train_lyrics = np.array(lyrics)[indices]\n",
    "# print(train_lyrics[0])\n",
    "# def read_corpus(_data, tokens_only=False):\n",
    "#     i = 0\n",
    "#     for key,line in _data.items():\n",
    "#         if tokens_only:\n",
    "#             yield gensim.utils.simple_preprocess(line)\n",
    "#         else:\n",
    "#             # For training data, add tags\n",
    "#             yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "#         i+=1\n",
    "        \n",
    "# train_corpus = list(read_corpus(train_lyrics))\n",
    "# model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "# model.build_vocab(train_corpus)\n",
    "# %time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# def lyric2vec():\n",
    "    \n",
    "    \n",
    "\n",
    "d2v_model = Doc2Vec.load(DOC2VEC_PATH + DOC2VEC_FILE)\n",
    "print(d2v_model.infer_vector(train_lyrics[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2dconv_model():\n",
    "    embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SONG_LENGTH,\n",
    "                                trainable=True)\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SONG_LENGTH,))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "#     print(embedded_sequences.shape)\n",
    "    # add first conv filter\n",
    "    embedded_sequences = Reshape((MAX_SONG_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n",
    "    x = Conv2D(100, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling2D((MAX_SONG_LENGTH - 10 + 1, 1))(x)\n",
    "    # add second conv filter.\n",
    "    y = Conv2D(100, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    y = MaxPooling2D((MAX_SONG_LENGTH - 8 + 1, 1))(y)\n",
    "    # add third conv filter.\n",
    "    z = Conv2D(100, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    z = MaxPooling2D((MAX_SONG_LENGTH - 6 + 1, 1))(z)\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x,y,z])\n",
    "\n",
    "#     alpha = Dropout(0.5)(alpha)\n",
    "    \n",
    "    # flatted the pooled features.\n",
    "    alpha = Flatten()(alpha)\n",
    "    \n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "    \n",
    "#   alpha = Dense(50, activation='relu')(alpha)\n",
    "    \n",
    "    # predictions\n",
    "    preds = Dense(len(genre_index), activation='softmax')(alpha)\n",
    "    # build model\n",
    "    model = Model(sequence_input, preds)\n",
    "#     adadelta = optimizers.Adam()\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "WARNING:tensorflow:From /Users/Joe/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/Joe/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1300)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1300, 100)    2000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1300, 100, 1) 0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1296, 1, 100) 50100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1297, 1, 100) 40100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1298, 1, 100) 30100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 100)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 1, 300)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 300)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            1505        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,121,805\n",
      "Trainable params: 2,121,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model')\n",
    "# opt = tf.keras.optimizers.Adam(lr=learning_rate, clipnorm=max_grad_norm)\n",
    "# model = create_basic_cnn_model()\n",
    "# model = create_complex_cnn_model(False)\n",
    "checkpointer = ModelCheckpoint(filepath=BEST_WEIGHTS_FILE, \n",
    "                               monitor = 'val_acc',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "model = create_2dconv_model()\n",
    "# model = create_conv_lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input tensors to a Model must come from `tf.keras.Input`. Received: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x1a3da977b8> (missing previous layer metadata).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-629d71ba2096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m temp_model.compile(loss='categorical_crossentropy',\n\u001b[1;32m      4\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=['acc'])\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Create a cache for iterator get_next op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_get_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeakKeyDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     80\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_validate_graph_inputs_and_outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1633\u001b[0m                          \u001b[0;34m'must come from `tf.keras.Input`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m                          \u001b[0;34m'Received: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m                          ' (missing previous layer metadata).')\n\u001b[0m\u001b[1;32m   1636\u001b[0m       \u001b[0;31m# Check that x is an input tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input tensors to a Model must come from `tf.keras.Input`. Received: <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x1a3da977b8> (missing previous layer metadata)."
     ]
    }
   ],
   "source": [
    "temp_model = model.layers[:9]\n",
    "temp_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "temp_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Train on 1205 samples, validate on 595 samples\n",
      "WARNING:tensorflow:From /Users/Joe/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "1152/1205 [===========================>..] - ETA: 2s - loss: 1.5699 - acc: 0.2847 \n",
      "Epoch 00001: val_acc improved from -inf to 0.43361, saving model to best_weights.hdf5\n",
      "1205/1205 [==============================] - 76s 63ms/sample - loss: 1.5655 - acc: 0.2871 - val_loss: 1.3989 - val_acc: 0.4336\n",
      "Epoch 2/20\n",
      "1152/1205 [===========================>..] - ETA: 2s - loss: 1.2818 - acc: 0.5078 \n",
      "Epoch 00002: val_acc improved from 0.43361 to 0.53277, saving model to best_weights.hdf5\n",
      "1205/1205 [==============================] - 76s 63ms/sample - loss: 1.2833 - acc: 0.5071 - val_loss: 1.2514 - val_acc: 0.5328\n",
      "Epoch 3/20\n",
      " 896/1205 [=====================>........] - ETA: 16s - loss: 1.1358 - acc: 0.5781"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3cc36aed1df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             validation_split=VALIDATION_SPLIT)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training Model')\n",
    "model_details = model.fit(x_train, y_train,\n",
    "            batch_size=128,\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1,\n",
    "            validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "scores= model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, error):\n",
    "    cmap=plt.cm.Blues\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    print(classes)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "    plt.title('Average Recognition Accuracy: %.02f' %(error))\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def check_accuracy(model2,x_test,y_test):\n",
    "    print('%d,%d'%(len(x_test),len(y_test)))\n",
    "    y_pred = model2.predict(x_test,verbose=0)\n",
    "    matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    accuracy = np.sum(np.identity(len(genre_index))*matrix)/len(y_test)\n",
    "    print('Accuracy: %.2f' %(accuracy))\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(matrix,list(genre_index.keys()),accuracy)\n",
    "    plt.savefig(GRAPHS_DIR+'confusion_matrix_'+str(epochs)+'.png')\n",
    "    print('saved confusion matrix')\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'accuracy_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved accuracy')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'loss_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model,x_test,y_test)\n",
    "plot_data(model_details)\n",
    "save_model(model,MODEL_SAVE_FILE, MODEL_SAVE_WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## OLD MODELS ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_basic_cnn_model():\n",
    "#     embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "#                                 EMBEDDING_DIM,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 input_length=MAX_SONG_LENGTH,\n",
    "#                                 trainable=False)\n",
    "#     model = Sequential()\n",
    "#     model.add(embedding_layer)\n",
    "#     model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "#     model.add(keras.layers.GlobalMaxPooling1D())\n",
    "#     model.add(keras.layers.Dense(10, activation='relu'))\n",
    "#     model.add(keras.layers.Dense(len(genre_index), activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['acc'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_conv_lstm_model():\n",
    "#     embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "#                                 EMBEDDING_DIM,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 input_length=MAX_SONG_LENGTH,\n",
    "#                                 trainable=False)\n",
    "#     model_conv = Sequential()\n",
    "#     model_conv.add(embedding_layer)\n",
    "#     model_conv.add(Dropout(0.2))\n",
    "#     model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "#     model_conv.add(MaxPooling1D(pool_size=4))\n",
    "#     model_conv.add(LSTM(EMBEDDING_DIM))\n",
    "#     model_conv.add(Dense(len(genre_index), activation='softmax'))\n",
    "#     model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "#     return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_complex_cnn_model(embedding_trained):\n",
    "\n",
    "#     embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "#                                 EMBEDDING_DIM,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 input_length=MAX_SONG_LENGTH,\n",
    "#                                 trainable=embedding_trained)\n",
    "#     CONV1D_OUT = 256\n",
    "#     CONV1D_OUT2 = CONV1D_OUT//2\n",
    "#     CONV1D_OUT3 = CONV1D_OUT2//2\n",
    "#     OUT = 5\n",
    "#     MAX_POOLING_1 = 8\n",
    "#     MAX_POOLING_2 = 8\n",
    "#     MAX_POOLING_4 = 13 #when MAX_SONG_LENGTH = 1300\n",
    "#     # MAX_POOLING_3 = 47 #when MAX_SONG_LENGTH = 1300\n",
    "#     # MAX_POOLING_2 = 35 #when MAX_SONG_LENGTH = 1000\n",
    "#     # DROPOUT = 0.5\n",
    "\n",
    "#     # conv1d = single spatial convolution of 2d input (sequence of 1000 - 200 demention vectors)\n",
    "#     # maxpooling1d = randomly downsizes by pooling val\n",
    "#     # dropout = randomly zeros at dropout rate to avoid overfitting\n",
    "\n",
    "#     model2 = Sequential()\n",
    "#     model2.add(embedding_layer)\n",
    "\n",
    "#     model2.add(tf.keras.layers.Conv1D(CONV1D_OUT, OUT, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.MaxPooling1D(MAX_POOLING_1))\n",
    "#     model2.add(tf.keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "#     model2.add(tf.keras.layers.Conv1D(CONV1D_OUT2, OUT, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.MaxPooling1D(MAX_POOLING_2))\n",
    "#     model2.add(tf.keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "#     model2.add(tf.keras.layers.Conv1D(CONV1D_OUT2, OUT, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.MaxPooling1D(MAX_POOLING_3))\n",
    "#     model2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#     model2.add(tf.keras.layers.Dense(CONV1D_OUT3, activation='relu'))\n",
    "#     model2.add(tf.keras.layers.Dense(len(genre_index), activation='softmax'))\n",
    "\n",
    "#     # opt = tf.keras.optimizers.RMSprop(lr=learning_rate, clipnorm=max_grad_norm)\n",
    "#     model2.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['acc'])\n",
    "#     return model2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
