{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math as m\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import gensim \n",
    "from gensim.models import doc2vec, Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER VARIABLES\n",
    "#### Uncomment on server\n",
    "# device = device_lib.list_local_devices()\n",
    "# print(device)\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# backend.set_session(sess)\n",
    "\n",
    "# NO GPU so must downsize\n",
    "CPU=True\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "VALIDATION_SPLIT = 0.33\n",
    "TEST_SPLIT = 0.2\n",
    "learning_rate = .001\n",
    "max_grad_norm = 1.\n",
    "DROPOUT = 0.5\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# PATH CONSTANTS\n",
    "PICKLE_ROOT = 'data/lyrics/'\n",
    "PICKLE_INPUT = 'CNN_input.pickle' \n",
    "\n",
    "EMBEDDING_PATH = 'data/glove_embeddings/'\n",
    "EMBEDDING_FILE = 'glove.6B.'+str(EMBEDDING_DIM)+'d.txt'\n",
    "\n",
    "MODEL_DIR = 'saved_models/'\n",
    "MODEL_SAVE_FILE = MODEL_DIR+'cnn_model_1.1_'+str(epochs)+'.json'\n",
    "MODEL_SAVE_WEIGHTS_FILE = MODEL_DIR+'cnn_model_1.1_'+str(epochs)+'.h5'\n",
    "BEST_WEIGHTS_FILE = MODEL_DIR+'best_weights'+str(epochs)+'.hdf5'\n",
    "\n",
    "GRAPHS_DIR = 'graphs_out/'\n",
    "\n",
    "DOC2VEC_PATH = MODEL_DIR + 'doc2vec/'\n",
    "DOC2VEC_FILE = 'd2v.model'\n",
    "\n",
    "TEST_DIR = 'data/test/'\n",
    "TOCKENIZER_PATH = TEST_DIR+'token.pickle'\n",
    "\n",
    "# Default values - changed later\n",
    "MAX_SONG_LENGTH = 2500\n",
    "MAX_UNIQUE_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "# Elmo could improve the word embeddings - need more research\n",
    "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "print('loading embedding')\n",
    "try:\n",
    "    if not os.path.exists(EMBEDDING_PATH+EMBEDDING_FILE):\n",
    "        print('Embeddings not found, downloading now')\n",
    "        try:\n",
    "            print(os.system('pwd'))\n",
    "            os.system(' cd ' + DATA_PATH)\n",
    "            os.system(' mkdir ' + EMBEDDING_DIR)\n",
    "            os.system(' cd ' + EMBEDDING_DIR)\n",
    "            os.system(' wget http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            os.system(' unzip glove.6B.zip')\n",
    "            os.system(' cd ../..')\n",
    "        except:\n",
    "            print('not optimized for this operating system.')\n",
    "            print('please download: ')\n",
    "            print('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            print('Note: this may take a while')\n",
    "            sys.exit()\n",
    "except:\n",
    "    print('Do you have the word embeddings?')\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(EMBEDDING_PATH+EMBEDDING_FILE, encoding='utf-8') as emb_f:\n",
    "    for line in emb_f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "print('finished loading embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('loading pickles')\n",
    "pickle_data = pickle.load( open(PICKLE_ROOT + PICKLE_INPUT , \"rb\" ))\n",
    "lyrics = pickle_data['lyrics']\n",
    "lyrics_labels = pickle_data['lyrics_labels']\n",
    "unique_words_set = pickle_data['unique_words_set']\n",
    "genre_index = pickle_data['genre_index']\n",
    "MAX_SONG_LENGTH = round(pickle_data['longest_song'],-2)\n",
    "print('number of songs: %d' %(len(lyrics)))\n",
    "print('number of genres: %d' %(len(genre_index)))\n",
    "print('number of lyrics: %d' %(len(lyrics_labels)))\n",
    "print('number of unique words: %d' %(len(unique_words_set)))\n",
    "print('longest song: %d' %(MAX_SONG_LENGTH))\n",
    "print('finished loading pickles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_UNIQUE_WORDS = len(unique_words_set)\n",
    "# MAX_SONG_LENGTH = 1000\n",
    "# data preparing\n",
    "print('tokenizing')\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "sequences = tokenizer.texts_to_sequences(lyrics)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Unique words tokens %d' % (len(word_index)))\n",
    "\n",
    "data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SONG_LENGTH,padding='post')\n",
    "labels = keras.utils.to_categorical(np.asarray(lyrics_labels))\n",
    "\n",
    "pickle.dump(tokenizer, open(TOCKENIZER_PATH,\"wb\" ))\n",
    "\n",
    "print('finished tokenizing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model(nn_model,filename,weights_filename):\n",
    "    # serialize model to JSON\n",
    "    model_json = nn_model.to_json()\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    nn_model.save_weights(weights_filename)\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(filename,weights_filename):\n",
    "    # load json and create model\n",
    "    json_file = open(filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_filename)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_data(ind):\n",
    "    filename = 'recent_testdata_'+str(epochs)+'.pickle'\n",
    "    data_ind = {}\n",
    "    data_ind['indices'] = ind\n",
    "    pickle.dump( data_ind, open(PICKLE_ROOT+filename, \"wb\" ) )\n",
    "    print('saved test data to %s%s' %(PICKLE_ROOT,filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "save_test_data(indices)\n",
    "\n",
    "t_data = data[:int(data.shape[0]*.1)]\n",
    "t_labels = labels[:int(data.shape[0]*.1)]\n",
    "\n",
    "if not CPU:\n",
    "    num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "    x_test = data[:num_test_samples]\n",
    "    y_test = labels[:num_test_samples]\n",
    "    x_train = data[num_test_samples:]\n",
    "    y_train = labels[num_test_samples:]\n",
    "    \n",
    "else:\n",
    "    num_test_samples = int(TEST_SPLIT * t_data.shape[0])\n",
    "\n",
    "    x_test = t_data[:num_test_samples]\n",
    "    y_test = t_labels[:num_test_samples]\n",
    "    x_train = t_data[num_test_samples:]\n",
    "    y_train = t_labels[num_test_samples:]\n",
    "\n",
    "print('data tensor:', data.shape)\n",
    "print('test tensor:', x_test.shape)\n",
    "print('train tensor:', x_train.shape)\n",
    "print('valid splits: ', x_test.shape[0]+x_train.shape[0] == data.shape[0])\n",
    "print('label tensor:', labels.shape)\n",
    "print('test tensor:', y_test.shape)\n",
    "print('train tensor:', y_train.shape)\n",
    "print('valid splits: ', y_test.shape[0]+y_train.shape[0] == data.shape[0])\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "unique_words_count = min(MAX_UNIQUE_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((unique_words_count, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_UNIQUE_WORDS:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # potentially can improve if OOV words are handled differently        \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class doc2vec(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec.load(DOC2VEC_PATH + DOC2VEC_FILE)\n",
    "# print(d2v_model.infer_vector(train_lyrics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2dconv_model():\n",
    "    embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SONG_LENGTH,\n",
    "                                trainable=True)\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SONG_LENGTH,))\n",
    "#     print((tf.Session().run(sequence_input)))\n",
    "# d2v_model.infer_vector()\n",
    "#     doc = d2v_model.infer_vector(tf.map_fn(np.array,sequence_input,dtype=np.ndarray).reshape(1300))\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "#     print(embedded_sequences.shape)\n",
    "    # add first conv filter\n",
    "    embedded_sequences = Reshape((MAX_SONG_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n",
    "    x = Conv2D(100, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling2D((MAX_SONG_LENGTH - 10 + 1, 1))(x)\n",
    "    # add second conv filter.\n",
    "    y = Conv2D(100, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    y = MaxPooling2D((MAX_SONG_LENGTH - 8 + 1, 1))(y)\n",
    "    # add third conv filter.\n",
    "    z = Conv2D(100, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    z = MaxPooling2D((MAX_SONG_LENGTH - 6 + 1, 1))(z)\n",
    "    # concate the conv layers\n",
    "#     a = layer(doc)(embedded_sequences)\n",
    "        \n",
    "    alpha = concatenate([x,y,z])\n",
    "#     alpha = Dropout(0.5)(alpha)    \n",
    "    # flatted the pooled features.\n",
    "    alpha = Flatten()(alpha)\n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "#   alpha = Dense(50, activation='relu')(alpha)\n",
    "    # predictions\n",
    "    preds = Dense(len(genre_index), activation='softmax')(alpha)\n",
    "    # build model\n",
    "    model = Model(sequence_input, preds)        \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building model')\n",
    "# opt = tf.keras.optimizers.Adam(lr=learning_rate, clipnorm=max_grad_norm)\n",
    "# model = create_basic_cnn_model()\n",
    "# model = create_complex_cnn_model(False)\n",
    "checkpointer = ModelCheckpoint(filepath=BEST_WEIGHTS_FILE, \n",
    "                               monitor = 'val_acc',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "model = create_2dconv_model()\n",
    "# model = create_conv_lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Model')\n",
    "model_details = model.fit(x_train, y_train,\n",
    "            batch_size=128,\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1,\n",
    "            validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "scores= model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, error):\n",
    "    cmap=plt.cm.Blues\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    print(classes)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "    plt.title('Average Recognition Accuracy: %.02f' %(error))\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def check_accuracy(model2,x_test,y_test):\n",
    "    print('%d,%d'%(len(x_test),len(y_test)))\n",
    "    y_pred = model2.predict(x_test,verbose=0)\n",
    "    matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    accuracy = np.sum(np.identity(len(genre_index))*matrix)/len(y_test)\n",
    "    print('Accuracy: %.2f' %(accuracy))\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(matrix,list(genre_index.keys()),accuracy)\n",
    "    plt.savefig(GRAPHS_DIR+'confusion_matrix_'+str(epochs)+'.png')\n",
    "    print('saved confusion matrix')\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'accuracy_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved accuracy')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'loss_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model,x_test,y_test)\n",
    "plot_data(model_details)\n",
    "save_model(model,MODEL_SAVE_FILE, MODEL_SAVE_WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused and not finished\n",
    "def visualize():\n",
    "    _ = glove2word2vec(EMBEDDING_PATH+EMBEDDING_FILE, EMBEDDING_PATH + 'word2vec_model.txt')\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(EMBEDDING_PATH + 'word2vec_model.txt')\n",
    "    layer_outputs = [layer.output for layer in model.layers][1:]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    layer_names = [layer.name for layer in model.layers][1:]\n",
    "    activations = activation_model.predict(x_train[0].reshape(1,1300))\n",
    "    print(layer_names[3])\n",
    "    print(activations[3].shape)\n",
    "    temp_vec = activations[3].reshape(1297,100)[0]\n",
    "    print(temp_vec)\n",
    "    print(w2v_model['to'])\n",
    "    w2v_model.similar_by_vector(temp_vec, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DOC2VEC ###############\n",
    "# train_lyrics = np.array(lyrics)[indices]\n",
    "# print(train_lyrics[0])\n",
    "# def read_corpus(_data, tokens_only=False):\n",
    "#     i = 0\n",
    "#     for key,line in _data.items():\n",
    "#         if tokens_only:\n",
    "#             yield gensim.utils.simple_preprocess(line)\n",
    "#         else:\n",
    "#             # For training data, add tags\n",
    "#             yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "#         i+=1\n",
    "        \n",
    "# train_corpus = list(read_corpus(train_lyrics))\n",
    "# model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "# model.build_vocab(train_corpus)\n",
    "# %time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# def lyric2vec():\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
