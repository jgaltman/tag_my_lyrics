{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math as m\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import gensim \n",
    "from gensim.models import doc2vec, Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER VARIABLES\n",
    "CPU=False\n",
    "epochs = 5\n",
    "\n",
    "#CONSTANTS\n",
    "VALIDATION_SPLIT = 0.33\n",
    "TEST_SPLIT = 0.2\n",
    "learning_rate = .001\n",
    "max_grad_norm = 1.\n",
    "DROPOUT = 0.5\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# PATH CONSTANTS\n",
    "PICKLE_ROOT = 'data/lyrics/'\n",
    "PICKLE_INPUT = 'CNN_input.pickle' \n",
    "\n",
    "EMBEDDING_PATH = 'data/glove_embeddings/'\n",
    "EMBEDDING_FILE = 'glove.6B.'+str(EMBEDDING_DIM)+'d.txt'\n",
    "\n",
    "MODEL_DIR = 'saved_models/'\n",
    "MODEL_SAVE_FILE = 'cnn_model_1.1_'+str(epochs)+'.json'\n",
    "MODEL_SAVE_WEIGHTS_FILE = 'cnn_model_1.1_'+str(epochs)+'.h5'\n",
    "BEST_WEIGHTS_FILE = 'best_weights'+str(epochs)+'.hdf5'\n",
    "\n",
    "GRAPHS_DIR = 'graphs_out/'\n",
    "\n",
    "DOC2VEC_PATH = MODEL_DIR + 'doc2vec/'\n",
    "DOC2VEC_FILE = 'd2v.model'\n",
    "\n",
    "TEST_DIR = 'data/test/'\n",
    "TOCKENIZER_PATH = TEST_DIR+'token.pickle'\n",
    "\n",
    "# Default values - changed later\n",
    "MAX_SONG_LENGTH = 2500\n",
    "MAX_UNIQUE_WORDS = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(nn_model,filename,weights_filename):\n",
    "    # serialize model to JSON\n",
    "    model_json = nn_model.to_json()\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    nn_model.save_weights(weights_filename)\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename,weights_filename):\n",
    "    # load json and create model\n",
    "    json_file = open(filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_filename)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_data(ind):\n",
    "    filename = 'recent_testdata_'+str(epochs)+'.pickle'\n",
    "    data_ind = {}\n",
    "    data_ind['indices'] = ind\n",
    "    pickle.dump( data_ind, open(PICKLE_ROOT+filename, \"wb\" ) )\n",
    "    print('saved test data to %s%s' %(PICKLE_ROOT,filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, error):\n",
    "    cmap=plt.cm.Blues\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    print(classes)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "    plt.title('Average Recognition Accuracy: %.02f' %(error))\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model2,x_test,y_test):\n",
    "    print('%d,%d'%(len(x_test),len(y_test)))\n",
    "    y_pred = model2.predict(x_test,verbose=0)\n",
    "    matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    accuracy = np.sum(np.identity(len(genre_index))*matrix)/len(y_test)\n",
    "    print('Accuracy: %.2f' %(accuracy))\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(matrix,list(genre_index.keys()),accuracy)\n",
    "    plt.savefig(GRAPHS_DIR+'confusion_matrix_'+str(epochs)+'.png')\n",
    "    print('saved confusion matrix')\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'accuracy_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved accuracy')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "    plt.savefig(GRAPHS_DIR+'loss_'+str(epochs)+'.png')\n",
    "    plt.clf()\n",
    "    print('saved loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding\n",
      "finished loading embedding\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "# Elmo could improve the word embeddings - need more research\n",
    "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "print('loading embedding')\n",
    "try:\n",
    "    if not os.path.exists(EMBEDDING_PATH+EMBEDDING_FILE):\n",
    "        print('Embeddings not found, downloading now')\n",
    "        try:\n",
    "            print(os.system('pwd'))\n",
    "            os.system(' cd ' + DATA_PATH)\n",
    "            os.system(' mkdir ' + EMBEDDING_DIR)\n",
    "            os.system(' cd ' + EMBEDDING_DIR)\n",
    "            os.system(' wget http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            os.system(' unzip glove.6B.zip')\n",
    "            os.system(' cd ../..')\n",
    "        except:\n",
    "            print('not optimized for this operating system.')\n",
    "            print('please download: ')\n",
    "            print('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "            print('Note: this may take a while')\n",
    "            sys.exit()\n",
    "except:\n",
    "    print('Do you have the word embeddings?')\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(EMBEDDING_PATH+EMBEDDING_FILE, encoding='utf-8') as emb_f:\n",
    "    for line in emb_f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "print('finished loading embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickles\n",
      "number of songs: 22500\n",
      "number of genres: 5\n",
      "number of lyrics: 22500\n",
      "number of unique words: 54404\n",
      "longest song: 1300\n",
      "finished loading pickles\n"
     ]
    }
   ],
   "source": [
    "print('loading pickles')\n",
    "pickle_data = pickle.load( open(PICKLE_ROOT + PICKLE_INPUT , \"rb\" ))\n",
    "lyrics = pickle_data['lyrics']\n",
    "lyrics_labels = pickle_data['lyrics_labels']\n",
    "unique_words_set = pickle_data['unique_words_set']\n",
    "genre_index = pickle_data['genre_index']\n",
    "MAX_SONG_LENGTH = round(pickle_data['longest_song'],-2)\n",
    "print('number of songs: %d' %(len(lyrics)))\n",
    "print('number of genres: %d' %(len(genre_index)))\n",
    "print('number of lyrics: %d' %(len(lyrics_labels)))\n",
    "print('number of unique words: %d' %(len(unique_words_set)))\n",
    "print('longest song: %d' %(MAX_SONG_LENGTH))\n",
    "print('finished loading pickles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n",
      "Unique words tokens 54391\n",
      "finished tokenizing\n"
     ]
    }
   ],
   "source": [
    "# MAX_UNIQUE_WORDS = len(unique_words_set)\n",
    "# MAX_SONG_LENGTH = 1000\n",
    "# data preparing\n",
    "print('tokenizing')\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "sequences = tokenizer.texts_to_sequences(lyrics)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Unique words tokens %d' % (len(word_index)))\n",
    "\n",
    "data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SONG_LENGTH,padding='post')\n",
    "labels = keras.utils.to_categorical(np.asarray(lyrics_labels))\n",
    "\n",
    "pickle.dump(tokenizer, open(TOCKENIZER_PATH,\"wb\" ))\n",
    "\n",
    "print('finished tokenizing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved test data to data/lyrics/recent_testdata_5.pickle\n",
      "data tensor: (22500, 1300)\n",
      "test tensor: (4500, 1300)\n",
      "train tensor: (18000, 1300)\n",
      "valid splits:  True\n",
      "label tensor: (22500, 5)\n",
      "test tensor: (4500, 5)\n",
      "train tensor: (18000, 5)\n",
      "valid splits:  True\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "save_test_data(indices)\n",
    "\n",
    "t_data = data[:int(data.shape[0]*.1)]\n",
    "t_labels = labels[:int(data.shape[0]*.1)]\n",
    "\n",
    "if not CPU:\n",
    "    num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "    x_test = data[:num_test_samples]\n",
    "    y_test = labels[:num_test_samples]\n",
    "    x_train = data[num_test_samples:]\n",
    "    y_train = labels[num_test_samples:]\n",
    "    \n",
    "else:\n",
    "    num_test_samples = int(TEST_SPLIT * t_data.shape[0])\n",
    "\n",
    "    x_test = t_data[:num_test_samples]\n",
    "    y_test = t_labels[:num_test_samples]\n",
    "    x_train = t_data[num_test_samples:]\n",
    "    y_train = t_labels[num_test_samples:]\n",
    "\n",
    "print('data tensor:', data.shape)\n",
    "print('test tensor:', x_test.shape)\n",
    "print('train tensor:', x_train.shape)\n",
    "print('valid splits: ', x_test.shape[0]+x_train.shape[0] == data.shape[0])\n",
    "print('label tensor:', labels.shape)\n",
    "print('test tensor:', y_test.shape)\n",
    "print('train tensor:', y_train.shape)\n",
    "print('valid splits: ', y_test.shape[0]+y_train.shape[0] == data.shape[0])\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "unique_words_count = min(MAX_UNIQUE_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((unique_words_count, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_UNIQUE_WORDS:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # potentially can improve if OOV words are handled differently        \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2D Convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2dconv_model():\n",
    "    conv_size = 100\n",
    "    embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SONG_LENGTH,\n",
    "                                trainable=True)\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SONG_LENGTH,))\n",
    "    #     print((tf.Session().run(sequence_input)))\n",
    "# d2v_model.infer_vector()\n",
    "#     doc = d2v_model.infer_vector(tf.map_fn(np.array,sequence_input,dtype=np.ndarray).reshape(1300))\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "#     print(embedded_sequences.shape)\n",
    "    # add first conv filter\n",
    "    embedded_sequences = Reshape((MAX_SONG_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n",
    "    x = Conv2D(conv_size, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling2D((MAX_SONG_LENGTH - 10 + 1, 1))(x)\n",
    "    # add second conv filter.\n",
    "    y = Conv2D(conv_size, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    y = MaxPooling2D((MAX_SONG_LENGTH - 8 + 1, 1))(y)\n",
    "    # add third conv filter.\n",
    "    z = Conv2D(conv_size, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    z = MaxPooling2D((MAX_SONG_LENGTH - 6 + 1, 1))(z)\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x,y,z])\n",
    "\n",
    "#     alpha = Dropout(0.5)(alpha)\n",
    "    \n",
    "    # flatted the pooled features.\n",
    "    alpha = Flatten()(alpha)\n",
    "    \n",
    "    # dropout\n",
    "\n",
    "    alpha = Dropout(DROPOUT)(alpha)\n",
    "    \n",
    "    # predictions\n",
    "    preds = Dense(len(genre_index), activation='softmax')(alpha)\n",
    "    # build model\n",
    "    model = Model(sequence_input, preds)\n",
    "#     adadelta = optimizers.Adam()\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "WARNING:tensorflow:From c:\\users\\josh\\envs\\cuda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\josh\\envs\\cuda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1300)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1300, 100)    2000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1300, 100, 1) 0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1298, 1, 100) 30100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1297, 1, 100) 40100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1296, 1, 100) 50100       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 100)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 1, 300)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 300)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            1505        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,121,805\n",
      "Trainable params: 2,121,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model')\n",
    "# opt = tf.keras.optimizers.Adam(lr=learning_rate, clipnorm=max_grad_norm)\n",
    "# model = create_basic_cnn_model()\n",
    "# model = create_complex_cnn_model(False)\n",
    "checkpointer = ModelCheckpoint(filepath=BEST_WEIGHTS_FILE,\n",
    "                               monitor = 'val_acc',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "model = create_2dconv_model()\n",
    "# model = create_conv_lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Train on 12059 samples, validate on 5941 samples\n",
      "WARNING:tensorflow:From c:\\users\\josh\\envs\\cuda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "12032/12059 [============================>.] - ETA: 0s - loss: 1.1840 - acc: 0.4977\n",
      "Epoch 00001: val_acc improved from -inf to 0.61336, saving model to best_weights5.hdf5\n",
      "12059/12059 [==============================] - 49s 4ms/sample - loss: 1.1832 - acc: 0.4980 - val_loss: 0.9606 - val_acc: 0.6134\n",
      "Epoch 2/5\n",
      "12032/12059 [============================>.] - ETA: 0s - loss: 0.9347 - acc: 0.6223\n",
      "Epoch 00002: val_acc improved from 0.61336 to 0.63878, saving model to best_weights5.hdf5\n",
      "12059/12059 [==============================] - 51s 4ms/sample - loss: 0.9345 - acc: 0.6226 - val_loss: 0.8801 - val_acc: 0.6388\n",
      "Epoch 3/5\n",
      "12032/12059 [============================>.] - ETA: 0s - loss: 0.8130 - acc: 0.6762\n",
      "Epoch 00003: val_acc improved from 0.63878 to 0.65713, saving model to best_weights5.hdf5\n",
      "12059/12059 [==============================] - 53s 4ms/sample - loss: 0.8131 - acc: 0.6764 - val_loss: 0.8414 - val_acc: 0.6571\n",
      "Epoch 4/5\n",
      "12032/12059 [============================>.] - ETA: 0s - loss: 0.7174 - acc: 0.7221\n",
      "Epoch 00004: val_acc improved from 0.65713 to 0.66538, saving model to best_weights5.hdf5\n",
      "12059/12059 [==============================] - 50s 4ms/sample - loss: 0.7174 - acc: 0.7220 - val_loss: 0.8215 - val_acc: 0.6654\n",
      "Epoch 5/5\n",
      "12032/12059 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.7630\n",
      "Epoch 00005: val_acc did not improve from 0.66538\n",
      "12059/12059 [==============================] - 50s 4ms/sample - loss: 0.6289 - acc: 0.7627 - val_loss: 0.8255 - val_acc: 0.6625\n",
      "Test loss: 0.8595660009913975\n",
      "Test accuracy: 0.652\n"
     ]
    }
   ],
   "source": [
    "print('Training Model')\n",
    "model_details = model.fit(x_train, y_train,\n",
    "            batch_size=128,\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "            callbacks=[checkpointer],\n",
    "            verbose=1,\n",
    "            validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "scores= model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy, plot data, save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500,4500\n",
      "Accuracy: 0.65\n",
      "Confusion matrix, without normalization\n",
      "[[587  68 150  57  17]\n",
      " [ 61 453 204 118  40]\n",
      " [114 224 448  88  20]\n",
      " [ 55 163  86 622   1]\n",
      " [  6  59  27   8 824]]\n",
      "['Christian', 'Pop', 'Rock', 'Country', 'Rap']\n",
      "saved confusion matrix\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "saved accuracy\n",
      "saved loss\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(model,x_test,y_test)\n",
    "plot_data(model_details)\n",
    "save_model(model,MODEL_SAVE_FILE, MODEL_SAVE_WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading doc2vec model\n",
    "# train_lyrics = np.array(lyrics)[indices]\n",
    "# print(train_lyrics[0])\n",
    "# def read_corpus(_data, tokens_only=False):\n",
    "#     i = 0\n",
    "#     for key,line in _data.items():\n",
    "#         if tokens_only:\n",
    "#             yield gensim.utils.simple_preprocess(line)\n",
    "#         else:\n",
    "#             # For training data, add tags\n",
    "#             yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "#         i+=1\n",
    "        \n",
    "# train_corpus = list(read_corpus(train_lyrics))\n",
    "# model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "# model.build_vocab(train_corpus)\n",
    "# %time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# def lyric2vec():\n",
    "    \n",
    "    \n",
    "\n",
    "# d2v_model = Doc2Vec.load(DOC2VEC_PATH + DOC2VEC_FILE)\n",
    "# print(d2v_model.infer_vector(train_lyrics[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused and not finished\n",
    "def visualize():\n",
    "    _ = glove2word2vec(EMBEDDING_PATH+EMBEDDING_FILE, EMBEDDING_PATH + 'word2vec_model.txt')\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(EMBEDDING_PATH + 'word2vec_model.txt')\n",
    "    layer_outputs = [layer.output for layer in model.layers][1:]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    layer_names = [layer.name for layer in model.layers][1:]\n",
    "    activations = activation_model.predict(x_train[0].reshape(1,1300))\n",
    "    print(layer_names[3])\n",
    "    print(activations[3].shape)\n",
    "    temp_vec = activations[3].reshape(1297,100)[0]\n",
    "    print(temp_vec)\n",
    "    print(w2v_model['to'])\n",
    "    w2v_model.similar_by_vector(temp_vec, topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
