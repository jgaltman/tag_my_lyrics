{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Joe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Joe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "###### comment out on server#######\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#### Uncomment on server\n",
    "# device = device_lib.list_local_devices()\n",
    "# print(device)\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "MAX_SONG_LENGTH = 2500\n",
    "# MAX_NUM_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.2\n",
    "SONG_PER_GENRE = 4500\n",
    "learning_rate = .001\n",
    "max_grad_norm = 1.\n",
    "dropout = 0.5\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH CONSTANTS\n",
    "PICKLE_ROOT = 'data/lyrics/'\n",
    "CHRISTIAN_PATH = 'Christian.pickle'\n",
    "POP_PATH = 'Pop.pickle'\n",
    "ROCK_PATH = 'Rock.pickle'\n",
    "COUNTRY_PATH = 'Country.pickle'\n",
    "RAP_PATH = 'Rap.pickle'\n",
    "\n",
    "LYRIC_PATHS = [CHRISTIAN_PATH,POP_PATH,ROCK_PATH,COUNTRY_PATH,RAP_PATH]\n",
    "\n",
    "EMBEDDING_PATH = 'data/glove_embeddings/'\n",
    "EMBEDDING_FILE = 'glove.6B.'+str(EMBEDDING_DIM)+'d.txt'\n",
    "\n",
    "MODEL_SAVE_FILE = 'cnn_model_1.0.json'\n",
    "MODEL_SAVE_WEIGHTS_FILE = 'cnn_model_1.0.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding\n",
      "finished loading embedding\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "# Elmo could improve the word embeddings - need more research\n",
    "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "print('loading embedding')\n",
    "if not os.path.exists(EMBEDDING_PATH+EMBEDDING_FILE):\n",
    "    print('Embeddings not found, downloading now')\n",
    "    print(os.system('pwd'))\n",
    "    os.system(' cd ' + DATA_PATH)\n",
    "    os.system(' mkdir ' + EMBEDDING_DIR)\n",
    "    os.system(' cd ' + EMBEDDING_DIR)\n",
    "    os.system(' wget http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    os.system(' unzip glove.6B.zip')\n",
    "    os.system(' cd ../..')\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(EMBEDDING_PATH+EMBEDDING_FILE, encoding='utf-8') as emb_f:\n",
    "    for line in emb_f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "print('finished loading embedding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickles\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Christian.pickle\n",
      "6895\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Pop.pickle\n",
      "5818\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Rock.pickle\n",
      "5596\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Country.pickle\n",
      "5368\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Rap.pickle\n",
      "4747\n",
      "5\n",
      "{'Christian': 0, 'Pop': 1, 'Rock': 2, 'Country': 3, 'Rap': 4}\n",
      "finished loading pickles\n"
     ]
    }
   ],
   "source": [
    "# Pickle extraction\n",
    "# pickle looks like -> pickle_lyrics['lyrics'][('song_title', 'artist')]['lyrics']\n",
    "# or - > pickle_lyrics['genre']\n",
    "print('loading pickles')\n",
    "\n",
    "pickle_lyrics = []\n",
    "genre_index = {}\n",
    "for i,l_path in enumerate(LYRIC_PATHS):\n",
    "    if not os.path.exists(PICKLE_ROOT+l_path):\n",
    "        print('problem occured looking for %s' %(PICKLE_ROOT+l_path))\n",
    "        sys.exit()\n",
    "    print(os.getcwd()+PICKLE_ROOT+l_path)\n",
    "    loaded_lyrics = pickle.load(open(PICKLE_ROOT+l_path, \"rb\" ))\n",
    "    genre_index[loaded_lyrics['genre']] = i\n",
    "    pickle_lyrics.append(loaded_lyrics)\n",
    "    print(len(loaded_lyrics['lyrics']))\n",
    "\n",
    "print(len(pickle_lyrics))\n",
    "print(genre_index)\n",
    "print('finished loading pickles')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning data\n",
      "cleaning: Christian\n",
      "0: Never Gonna Let Me Go\n",
      "1000: By His Wounds\n",
      "2000: Living For You\n",
      "3000: Whole World\n",
      "4000: I Manipulate\n",
      "hit max songs: 4500\n",
      "songs left out: 2395\n",
      "cleaning: Pop\n",
      "0: Emotional\n",
      "1000: Day Is Done\n",
      "2000: Roll Witchu\n",
      "3000: Broken Glass\n",
      "4000: Bitchin' Summer\n",
      "hit max songs: 4500\n",
      "songs left out: 1318\n",
      "cleaning: Rock\n",
      "0: Pleasure\n",
      "1000: Interesting Drug\n",
      "2000: Vilify\n",
      "3000: Almost (Sweet Music)\n",
      "4000: Unbelievers\n",
      "hit max songs: 4500\n",
      "songs left out: 1096\n",
      "cleaning: Country\n",
      "0: Here Comes My Baby\n",
      "1000: Passionate Kisses\n",
      "2000: My Favorite Memory\n",
      "3000: Tattoos On This Town\n",
      "4000: My Old Man\n",
      "hit max songs: 4500\n",
      "songs left out: 868\n",
      "cleaning: Rap\n",
      "0: Can't Tell\n",
      "1000: Dollar Sign\n",
      "2000: U Don't Know Me\n",
      "3000: Barbie Dreams\n",
      "4000: Haters Listen Up!\n",
      "hit max songs: 4500\n",
      "songs left out: 247\n",
      "\n",
      "\n",
      "\n",
      "number of songs: 22500\n",
      "number of lyrics: 22500\n",
      "number of unique words: 54404\n",
      "finished cleaning data\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_and_punct(article):\n",
    "    final_article = []\n",
    "    word_tokens = tokenizer.tokenize(article)\n",
    "    filtered_article = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_article = [x.lower() for x in filtered_article]\n",
    "    filtered_song = ' '.join(filtered_article)\n",
    "    return filtered_song, filtered_article\n",
    "\n",
    "def clean_data(data):\n",
    "    song_list = []\n",
    "    unique_words_list = []\n",
    "    count = 0\n",
    "    for key, song_info in data['lyrics'].items():\n",
    "        title, artist = key\n",
    "        inner_title = song_info['title']\n",
    "        if count%1000==0:\n",
    "            print('%d: %s' %(count, inner_title))\n",
    "        inner_artist = song_info['artist']\n",
    "        song_lyrics = song_info['lyrics']\n",
    "    \n",
    "#       song_lyrics_norm = re.sub(r'[^a-zA-Z0-9-\\']', ' ', song_lyrics).strip()\n",
    "#       song_lyrics_split = song_lyrics_norm.lower().split()\n",
    "        song_lyrics_norm, song_lyrics_split = remove_stop_and_punct(song_lyrics)\n",
    "    \n",
    "        if len(song_lyrics_split) <= MAX_SONG_LENGTH:       \n",
    "            song_list.append(song_lyrics_norm)\n",
    "            unique_words_list = list(set(unique_words_list + song_lyrics_split))\n",
    "        count+=1\n",
    "        \n",
    "        if count >= SONG_PER_GENRE:\n",
    "            print('hit max songs: %d' %(SONG_PER_GENRE))\n",
    "            print('songs left out: %d' %(len(data['lyrics'])-SONG_PER_GENRE))\n",
    "            return song_list, unique_words_list\n",
    "       \n",
    "    return song_list, unique_words_list\n",
    "# initial data pre-processing\n",
    "# assuming a list of tokenized data \n",
    "# vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_len)\n",
    "print('cleaning data')\n",
    "lyrics = []\n",
    "lyrics_labels = []\n",
    "unique_words_set = []\n",
    "for data in pickle_lyrics:\n",
    "    genre = data['genre']\n",
    "    print('cleaning: %s' %(genre))\n",
    "    song_list, unique_words = clean_data(data)\n",
    "    unique_words_set = list(set(unique_words_set+unique_words))\n",
    "    song_labels = [genre_index[genre]]*len(song_list)\n",
    "    \n",
    "    lyrics = lyrics + song_list\n",
    "    lyrics_labels = lyrics_labels + song_labels\n",
    "print('\\n\\n')\n",
    "print('number of songs: %d' %(len(lyrics)))\n",
    "print('number of lyrics: %d' %(len(lyrics_labels)))\n",
    "print('number of unique words: %d' %(len(unique_words_set)))\n",
    "print('finished cleaning data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n",
      "Unique words tokens 54391\n",
      "finished tokenizing\n"
     ]
    }
   ],
   "source": [
    "# MAX_UNIQUE_WORDS = len(unique_words_set)\n",
    "MAX_UNIQUE_WORDS = 20000\n",
    "# MAX_SONG_LENGTH = 1000\n",
    "\n",
    "# data preparing\n",
    "print('tokenizing')\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "sequences = tokenizer.texts_to_sequences(lyrics)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Unique words tokens %d' % (len(word_index)))\n",
    "\n",
    "data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SONG_LENGTH)\n",
    "labels = keras.utils.to_categorical(np.asarray(lyrics_labels))\n",
    "\n",
    "print('finished tokenizing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0   57   32    1 1409  166 1922   19  201\n",
      " 1441  112   97    1  379   16  309 5486   12    1 1065   11   18 7859\n",
      "  156    1  167   64  482 2712    1  224 1061 1805  534   46    1    5\n",
      "   84   95    1    5    3   17   42   13   11    1 2613  470 1741  758\n",
      " 5561 1741  172   84   95    1    5    3   17   42   13   11   13   11\n",
      "    3   17   42   13   11   13   11    3   17   42   13   11   28   64\n",
      " 2467   13    9  181  697  181  697    1   36  487    3  515 1457  895\n",
      "    1  201 2448 2258  282  575  959   49   32 1327    1  222   20  424\n",
      "    1    1  224 1061 1805  534   46    1    5   84   95    1    5    3\n",
      "   17   42   13   11    1 2613  470 1741  758 5561 1741  172   84   95\n",
      "    1    5    3   17   42   13   11    8    3  496   49   32   91   64\n",
      "  169    8    3  496   49   32   91   64  169   73    3   13   11  838\n",
      "    1  224 1061 1805  534   46    1    5   84   95    1    5    3   17\n",
      "   42   13   11    1 2613  470 1741  758 5561 1741  172   84   95    1\n",
      "    5    3   17   42   13   11   13   11    3   17   42   13   11   13\n",
      "   11    3   17   42   13   11   13   11    3   17   42   13   11   13\n",
      "   11    3   17   42   13   11   13   11    3   17   42   13   11   13\n",
      "   11    3   17   42   13   11]\n",
      "[1. 0. 0. 0. 0.]\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0    31  1383   972\n",
      "   160 10999   452     5     1   430    35    67   154  1383   368  2046\n",
      "   175     8     1   174     3     7   145  1683  2861     5     1   430\n",
      "     3    67   154    69   546    16     3    67    67     3    67     3\n",
      "    67   154  1250     1   205    33    67     1   205     3    67   154\n",
      "   298   766  1082    74 10711   484  2055     3    67    21  2031    95\n",
      "   184     5     7    63   416    90     1  1251  8163  1976     2     1\n",
      "    25   182     4  5685  1593  5685  1593     1  1353  1953     1   999\n",
      "  1788     2   180    11   673    11   673    47    66     4 13583     1\n",
      "   514   235    32     4     1  3179  4236  3179  4236    13     9  2046\n",
      "   235    27    90    90    90     1    34    27    27    90    90    90\n",
      "     1    24    27    27    90    90    90     1    24    27     1    24\n",
      "    27    27  4696    90    90    90     1     7   253  9973     2     1\n",
      "     5    33   712   484   193  2154     2     1    33    95    12     1\n",
      "     7   337   200    28  2188   119 11346     2     1  1161  2041   185\n",
      "    89    29  5430 18140  1195     9   897     1    17   797    96 18764\n",
      "    57   525  1489  8801  1143   521  3481     4  8913  8913   617    77\n",
      "  4011 10975   712    11  1260     1   176    83    10     1     9  2854\n",
      "     1    16  2854     1   712  2113     9  1758     1    16     1  8455\n",
      "     1  1242   536  6783   610    39   457    14   115    14    13    11\n",
      "  5351  6997   610     3    67   154    69   546    16     3    67    67\n",
      "     3    67     3    67   154  1250     1   205    33    67     1   205\n",
      "     3    67   154   298   766  1082    74 10711   484  2055   484  2055\n",
      "    21  2031    95   184     5     7    63   416    90     1  1251  8163\n",
      "  1976     2     1    25   182     4  5685  1593     1  1353  1953     1\n",
      "   999  1788     2   180    11   673    47    66     4 13583     1   514\n",
      "   235    32     4     1  3179  4236  3179  4236    13     9  2046   235\n",
      "    27    90    90    90    27    27    90    90    90     1    24    27\n",
      "    90    90    90     1    24    27    27    10    90    90    90   610\n",
      "   146   923   176   154    25   475    56   146   923    21   301   379\n",
      "   120     1    42   136    47  3766  1375  5725    98   386   417  4555\n",
      "  1375   195   116  3611     1    24  6832    49   210    77  4071    10\n",
      "   245   120  3332    10     3    93     5    10    19   616     1     5\n",
      "    97  2420    12     1    93    11 17377    20   438   171     1  1259\n",
      "  4092     3    67     1   177   597   177     3    67     1   177   597\n",
      "   177     1  1346     1    88   234  1346     1  1346     1   176   294\n",
      "    32    78   177   634    10   586     1    88   586   676  1480     1\n",
      "    88    57  1480     2     1    67     1  3458     1  8409   339  2151\n",
      "  3751     1    24    83     2    61  3991  4721  4721    12   797     9\n",
      "  1561  1561     1   590     1    67    73    73     7   122   133  2500\n",
      "  2500     3    67   154    69   546    16     3    67    67     3    67\n",
      "   154  1250     1   205    33    67   205     3    67   154   298   766\n",
      "  1082    74 10711   484  2055    21  2031    95   184     5     7    63\n",
      "   416     1  1251  8163  1976     2     1    25   182     4  5685  1593\n",
      "   434  5685  1593     1  1353  1953     1   999  1788     2   180    11\n",
      "   673   673    47    66     4 13583     1   514   235    32     4     1\n",
      "  3179  4236    13     9  2046   235    27    90    90    90    27    27\n",
      "    90    90    90     1    24    27    90    90    90     1    24    27\n",
      "    27    90    90    90    10    13    25   146  2358  4696    47   461\n",
      "   146 15878    47   461   146 15878     1     7  4788    10     1    24\n",
      "   146  4969     1     7    60   586   577     1     7    60   586   577\n",
      "    12     1    24   393  1003    62    24   393  1003   179  3703     7\n",
      "  2653   446     1   185    33  2089   863  2154     1  3315   208  4102\n",
      "     1  1194  2315     1    24   122   530     1    24  1208     1    24\n",
      "  1685  1208     1   572  1146   572     1   165  6388    84   531     4\n",
      "  6485   592    84   208   531    49   148  1438   328     1    25  7994\n",
      "  2261 10156  1930   325  2555  1930   325    21   574  1863  6455  5144\n",
      "     7  1863 12104    54  1068  3239   642  1721    22    28  1972  1149\n",
      " 11839   291  1018  1165  2588   260   151    23  4438 16079   944    27\n",
      "   171     1    28  1295   462     3    67   154    69   546    16     3\n",
      "    67    67     3    67   154  1250     1   205    33    67     3    67\n",
      "   154   298   766  1082    74 10711   484  2055    21  2031    95   184\n",
      "     5     7    63   416     1  1251  8163  1976     2     1    25   182\n",
      "     4  5685  1593   434  5685  1593     1  1353  1953     1   999  1788\n",
      "     2   180    11   673   673    47    66     4 13583     1   514   235\n",
      "    32     4     1  3179  4236    13     9  2046 11359    71    27    90\n",
      "    90    90    27    27    90    90    90     1    24    27    90    90\n",
      "    90     1    24    27    90    90    90     1    24    27    90    90\n",
      "    90    10    80    80]\n",
      "[0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(labels[0])\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "print(data[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data tensor: (22500, 1000)\n",
      "test tensor: (450, 1000)\n",
      "validate tensor: (360, 1000)\n",
      "train tensor: (1440, 1000)\n",
      "valid splits:  False\n",
      "label tensor: (22500, 5)\n",
      "test tensor: (450, 5)\n",
      "validate tensor: (360, 5)\n",
      "train tensor: (1440, 5)\n",
      "valid splits:  False\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "\n",
    "\n",
    "t_data = data[:int(data.shape[0]*.1)]\n",
    "t_labels = labels[:int(data.shape[0]*.1)]\n",
    "\n",
    "# NO GPU so must downsize\n",
    "CPU=True\n",
    "if not CPU:\n",
    "    num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * (data.shape[0]-num_test_samples))\n",
    "\n",
    "    x_test = data[:num_test_samples]\n",
    "    y_test = labels[:num_test_samples]\n",
    "    x_val = data[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    y_val = labels[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    x_train = data[num_test_samples+num_validation_samples:]\n",
    "    y_train = labels[num_test_samples+num_validation_samples:]\n",
    "    \n",
    "else:\n",
    "    num_test_samples = int(TEST_SPLIT * t_data.shape[0])\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * (t_data.shape[0]-num_test_samples))\n",
    "\n",
    "    x_test = t_data[:num_test_samples]\n",
    "    y_test = t_labels[:num_test_samples]\n",
    "    x_val = t_data[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    y_val = t_labels[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    x_train = t_data[num_test_samples+num_validation_samples:]\n",
    "    y_train = t_labels[num_test_samples+num_validation_samples:]\n",
    "    \n",
    "print('data tensor:', data.shape)\n",
    "print('test tensor:', x_test.shape)\n",
    "print('validate tensor:', x_val.shape)\n",
    "print('train tensor:', x_train.shape)\n",
    "print('valid splits: ', x_test.shape[0]+x_val.shape[0]+x_train.shape[0] == data.shape[0])\n",
    "print('label tensor:', labels.shape)\n",
    "print('test tensor:', y_test.shape)\n",
    "print('validate tensor:', y_val.shape)\n",
    "print('train tensor:', y_train.shape)\n",
    "print('valid splits: ', y_test.shape[0]+y_val.shape[0]+y_train.shape[0] == data.shape[0])\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "unique_words_count = min(MAX_UNIQUE_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((unique_words_count, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_UNIQUE_WORDS:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # potentially can improve if OOV words are handled differently        \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SONG_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model(filename,weights_filename):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(weights_filename)\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(filename,weights_filename):\n",
    "    # load json and create model\n",
    "    json_file = open(filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_filename)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 200)         4000000   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 996, 128)          128128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,309,381\n",
      "Trainable params: 309,381\n",
      "Non-trainable params: 4,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "sequence_input = tf.keras.layers.Input(shape=(MAX_SONG_LENGTH,))\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#Model 1\n",
    "l_cov1= tf.keras.layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = tf.keras.layers.MaxPooling1D(5)(l_cov1)\n",
    "l_drop1= tf.keras.layers.Dropout(dropout)(l_pool1)\n",
    "l_cov2 = tf.keras.layers.Conv1D(128, 5, activation='relu')(l_drop1)\n",
    "l_pool2 = tf.keras.layers.MaxPooling1D(5)(l_cov2)\n",
    "l_drop2 = tf.keras.layers.Dropout(dropout)(l_pool2)\n",
    "l_cov3 = tf.keras.layers.Conv1D(128, 5, activation='relu')(l_drop2)\n",
    "l_pool3 = tf.keras.layers.MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = tf.keras.layers.Flatten()(l_pool3)\n",
    "l_dense = tf.keras.layers.Dense(128, activation='relu')(l_flat)\n",
    "preds = tf.keras.layers.Dense(len(genre_index), activation='softmax')(l_dense)\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate, clipnorm = max_grad_norm)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1440 samples, validate on 360 samples\n",
      "Epoch 1/100\n",
      "1440/1440 [==============================] - 39s 27ms/sample - loss: 1.5736 - acc: 0.3111 - val_loss: 1.5357 - val_acc: 0.2444\n",
      "Epoch 2/100\n",
      "1440/1440 [==============================] - 36s 25ms/sample - loss: 1.3733 - acc: 0.3722 - val_loss: 1.4459 - val_acc: 0.4194\n",
      "Epoch 3/100\n",
      "1440/1440 [==============================] - 36s 25ms/sample - loss: 1.2523 - acc: 0.4701 - val_loss: 1.2949 - val_acc: 0.4361\n",
      "Epoch 4/100\n",
      "1440/1440 [==============================] - 39s 27ms/sample - loss: 1.1539 - acc: 0.5146 - val_loss: 1.2403 - val_acc: 0.4833\n",
      "Epoch 5/100\n",
      "1440/1440 [==============================] - 43s 30ms/sample - loss: 1.0725 - acc: 0.5333 - val_loss: 1.2054 - val_acc: 0.4778\n",
      "Epoch 6/100\n",
      "1440/1440 [==============================] - 37s 26ms/sample - loss: 1.0210 - acc: 0.5611 - val_loss: 1.2090 - val_acc: 0.4806\n",
      "Epoch 7/100\n",
      "1440/1440 [==============================] - 37s 26ms/sample - loss: 0.9482 - acc: 0.6146 - val_loss: 1.1211 - val_acc: 0.5306\n",
      "Epoch 8/100\n",
      "1408/1440 [============================>.] - ETA: 0s - loss: 0.8957 - acc: 0.6278"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4ac329748025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             validation_data=(x_val, y_val))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/hw/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_details = model.fit(x_train, y_train,\n",
    "            epochs=100,\n",
    "            shuffle=True,\n",
    "            verbose=1,\n",
    "            validation_data=(x_val, y_val))\n",
    "\n",
    "scores = model.evaluate(x_test,y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(MODEL_SAVE_FILE, MODEL_SAVE_WEIGHTS_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
