{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "MAX_SONG_LENGTH = 2500\n",
    "# MAX_NUM_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.3\n",
    "TEST_SPLIT = 0.2\n",
    "learning_rate = .001\n",
    "max_grad_norm = 1.\n",
    "dropout = 0.5\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH CONSTANTS\n",
    "PICKLE_ROOT = 'data/lyrics/'\n",
    "CHRISTIAN_PATH = 'Christian.pickle'\n",
    "POP_PATH = 'Pop.pickle'\n",
    "ROCK_PATH = 'Rock.pickle'\n",
    "COUNTRY_PATH = 'Country.pickle'\n",
    "RAP_PATH = 'Rap.pickle'\n",
    "\n",
    "LYRIC_PATHS = [CHRISTIAN_PATH,POP_PATH,ROCK_PATH,COUNTRY_PATH,RAP_PATH]\n",
    "\n",
    "EMBEDDING_PATH = 'data/glove_embeddings/'\n",
    "EMBEDDING_FILE = 'glove.6B.'+str(EMBEDDING_DIM)+'d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "# Elmo could improve the word embeddings - need more research\n",
    "# elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "if not os.path.exists(EMBEDDING_PATH+EMBEDDING_FILE):\n",
    "    print('Embeddings not found, downloading now')\n",
    "    ! cd EMBEDDING_PATH\n",
    "    ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    ! unzip glove.6B.zip\n",
    "    ! cd ../..\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(EMBEDDING_PATH+EMBEDDING_FILE) as emb_f:\n",
    "    for line in emb_f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Christian.pickle\n",
      "10186\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Pop.pickle\n",
      "8618\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Rock.pickle\n",
      "8054\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Country.pickle\n",
      "7516\n",
      "/Users/Joe/Applications/OneDrive/School/Spring 2019 - senior/NLP/project/tag_my_lyricsdata/lyrics/Rap.pickle\n",
      "8247\n",
      "5\n",
      "{'Christian': 0, 'Pop': 1, 'Rock': 2, 'Country': 3, 'Rap': 4}\n"
     ]
    }
   ],
   "source": [
    "# Pickle extraction\n",
    "# pickle looks like -> pickle_lyrics['lyrics'][('song_title', 'artist')]['lyrics']\n",
    "# or - > pickle_lyrics['genre']\n",
    "pickle_lyrics = []\n",
    "genre_index = {}\n",
    "max_length = 0\n",
    "for i,l_path in enumerate(LYRIC_PATHS):\n",
    "    if not os.path.exists(PICKLE_ROOT+l_path):\n",
    "        print('problem occured looking for %s' %(PICKLE_ROOT+l_path))\n",
    "        sys.exit()\n",
    "    print(os.getcwd()+PICKLE_ROOT+l_path)\n",
    "    loaded_lyrics = pickle.load(open(PICKLE_ROOT+l_path, \"rb\" ))\n",
    "    genre_index[loaded_lyrics['genre']] = i\n",
    "    pickle_lyrics.append(loaded_lyrics)\n",
    "    print(len(loaded_lyrics['lyrics']))\n",
    "    for key, song_info in loaded_lyrics['lyrics'].items():\n",
    "        if len(song_info['lyrics'].split()) > max_length:\n",
    "            max_length = len(song_info['lyrics'].split())\n",
    "#             print(key)\n",
    "#             print(max_length)\n",
    "#             print(i)\n",
    "print(len(pickle_lyrics))\n",
    "print(genre_index)\n",
    "# print(max_length)\n",
    "# print(pickle_lyrics[0]['lyrics']['Cabin Essence: Chorus', 'The Beach Boys']['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christian\n",
      "('Price Tag', \"Da' T.R.U.T.H.\")\n",
      "1104\n",
      "10186  :  6895\n",
      "Pop\n",
      "('Tropico', 'Lana Del Rey')\n",
      "1129\n",
      "8618  :  5818\n",
      "Rock\n",
      "('The Real Slim Shady', 'Eminem')\n",
      "1013\n",
      "8054  :  5596\n",
      "Country\n",
      "('The Haircut Song', 'Ray Stevens')\n",
      "877\n",
      "7516  :  5368\n",
      "Rap\n",
      "('Mortal Man', 'Kendrick Lamar')\n",
      "2234\n",
      "8247  :  4747\n",
      "64893\n"
     ]
    }
   ],
   "source": [
    "def check_validity(data):\n",
    "    valid_count = 0\n",
    "    max_len_key = ''\n",
    "    max_len = 0\n",
    "    total_words = []\n",
    "    for key, song_info in data['lyrics'].items():\n",
    "        title, artist = key\n",
    "        inner_title = song_info['title']\n",
    "        inner_artist = song_info['artist']\n",
    "        song_lyrics = song_info['lyrics']\n",
    "        song_lyrics_norm = re.sub(r'[^a-zA-Z0-9-\\']', ' ', song_lyrics).strip()\n",
    "        song_lyrics_split = song_lyrics_norm.split()         \n",
    "        if title == inner_title and artist == inner_artist and len(song_lyrics_split) <= MAX_SONG_LENGTH:\n",
    "            if len(song_lyrics_split) > max_len:\n",
    "                max_len = len(song_lyrics_split)\n",
    "                max_len_key = key\n",
    "            valid_count+=1\n",
    "            total_words = list(set(total_words+song_lyrics_split))\n",
    "    print(max_len_key)\n",
    "    print(max_len)\n",
    "    return valid_count, total_words\n",
    "\n",
    "for data in pickle_lyrics:\n",
    "    print(data['genre'])\n",
    "    total_songs = len(data['lyrics'])\n",
    "    total_words_set = []\n",
    "    valid, total_words = check_validity(data)\n",
    "    total_words_set  = list(set(total_words_set+total_words))\n",
    "    print(total_songs, ' : ', valid)\n",
    "print(len(total_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28424\n",
      "28424\n"
     ]
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    song_list = []\n",
    "    for key, song_info in data['lyrics'].items():\n",
    "        title, artist = key\n",
    "        inner_title = song_info['title']\n",
    "        inner_artist = song_info['artist']\n",
    "        song_lyrics = song_info['lyrics']\n",
    "        song_lyrics_norm = re.sub(r'[^a-zA-Z0-9-\\']', ' ', song_lyrics).strip()\n",
    "        song_lyrics_split = song_lyrics_norm.split()         \n",
    "        if title == inner_title and artist == inner_artist and len(song_lyrics_split) <= MAX_SONG_LENGTH:       \n",
    "            song_list.append(song_lyrics_norm)\n",
    "            \n",
    "    return song_list\n",
    "# initial data pre-processing\n",
    "# assuming a list of tokenized data \n",
    "# vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_len)\n",
    "\n",
    "lyrics = []\n",
    "lyrics_labels = []\n",
    "for data in pickle_lyrics:\n",
    "    genre = data['genre']\n",
    "#     for key, song_info in data['lyrics'].items():\n",
    "#         song_lyrics = song_info['lyrics']\n",
    "#         song_lyrics_norm = re.sub(r'[^a-zA-Z0-9-\\']', ' ', song_lyrics).strip()\n",
    "#         song_lyrics_split = song_lyrics_norm.split() \n",
    "#         print(song_lyrics)\n",
    "#         print()\n",
    "#         print(song_lyrics_norm)\n",
    "#         print()\n",
    "#         print(song_lyrics_split)\n",
    "    song_list = clean_data(data)\n",
    "    \n",
    "    song_labels = [genre_index[genre]]*len(song_list)\n",
    "    \n",
    "    lyrics = lyrics + song_list\n",
    "    lyrics_labels = lyrics_labels + song_labels\n",
    "print(len(lyrics))\n",
    "print(len(lyrics_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words tokens 64781\n"
     ]
    }
   ],
   "source": [
    "MAX_UNIQUE_WORDS = len(total_words_set)\n",
    "# data preparing\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "sequences = tokenizer.texts_to_sequences(lyrics)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Unique words tokens %d' % (len(word_index)))\n",
    "\n",
    "data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SONG_LENGTH)\n",
    "labels = keras.utils.to_categorical(np.asarray(lyrics_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data tensor: (28424, 2500)\n",
      "test tensor: (568, 2500)\n",
      "validate tensor: (682, 2500)\n",
      "train tensor: (1592, 2500)\n",
      "valid splits:  False\n",
      "label tensor: (28424, 5)\n",
      "test tensor: (568, 5)\n",
      "validate tensor: (682, 5)\n",
      "train tensor: (1592, 5)\n",
      "valid splits:  False\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "t_data = data[:int(data.shape[0]*.1)]\n",
    "t_labels = labels[:int(data.shape[0]*.1)]\n",
    "\n",
    "# NO GPU so must downsize\n",
    "macbook=True\n",
    "if not macbook:\n",
    "    num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * (data.shape[0]-num_test_samples))\n",
    "\n",
    "    x_test = data[:num_test_samples]\n",
    "    y_test = labels[:num_test_samples]\n",
    "    x_val = data[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    y_val = labels[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    x_train = data[num_test_samples+num_validation_samples:]\n",
    "    y_train = labels[num_test_samples+num_validation_samples:]\n",
    "    \n",
    "else:\n",
    "    num_test_samples = int(TEST_SPLIT * t_data.shape[0])\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * (t_data.shape[0]-num_test_samples))\n",
    "\n",
    "    x_test = t_data[:num_test_samples]\n",
    "    y_test = t_labels[:num_test_samples]\n",
    "    x_val = t_data[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    y_val = t_labels[num_test_samples:num_test_samples+num_validation_samples]\n",
    "    x_train = t_data[num_test_samples+num_validation_samples:]\n",
    "    y_train = t_labels[num_test_samples+num_validation_samples:]\n",
    "    \n",
    "print('data tensor:', data.shape)\n",
    "print('test tensor:', x_test.shape)\n",
    "print('validate tensor:', x_val.shape)\n",
    "print('train tensor:', x_train.shape)\n",
    "print('valid splits: ', x_test.shape[0]+x_val.shape[0]+x_train.shape[0] == data.shape[0])\n",
    "print('label tensor:', labels.shape)\n",
    "print('test tensor:', y_test.shape)\n",
    "print('validate tensor:', y_val.shape)\n",
    "print('train tensor:', y_train.shape)\n",
    "print('valid splits: ', y_test.shape[0]+y_val.shape[0]+y_train.shape[0] == data.shape[0])\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "unique_words_count = min(MAX_UNIQUE_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((unique_words_count, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_UNIQUE_WORDS:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # potentially can improve if OOV words are handled differently        \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = keras.layers.Embedding(unique_words_count,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SONG_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 1592 samples, validate on 682 samples\n",
      "1592/1592 [==============================] - 133s 83ms/sample - loss: 1.5366 - acc: 0.3028 - val_loss: 1.5136 - val_acc: 0.3065\n",
      "Test loss: 1.5101940615076415\n",
      "Test accuracy: 0.30985916\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "sequence_input = tf.keras.layers.Input(shape=(MAX_SONG_LENGTH,))\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#Model 1\n",
    "l_cov1= tf.keras.layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = tf.keras.layers.MaxPooling1D(5)(l_cov1)\n",
    "l_drop1= tf.keras.layers.Dropout(0.2)(l_pool1)\n",
    "l_cov2 = tf.keras.layers.Conv1D(128, 5, activation='relu')(l_drop1)\n",
    "l_pool2 = tf.keras.layers.MaxPooling1D(5)(l_cov2)\n",
    "l_drop2 = tf.keras.layers.Dropout(0.2)(l_pool2)\n",
    "l_cov3 = tf.keras.layers.Conv1D(128, 5, activation='relu')(l_drop2)\n",
    "l_pool3 = tf.keras.layers.MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = tf.keras.layers.Flatten()(l_pool3)\n",
    "l_dense = tf.keras.layers.Dense(128, activation='relu')(l_flat)\n",
    "preds = tf.keras.layers.Dense(len(genre_index), activation='softmax')(l_dense)\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate, clipnorm = max_grad_norm)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_details = model.fit(x_train, y_train,\n",
    "            epochs=1,\n",
    "            shuffle=True,\n",
    "            verbose=1,\n",
    "            validation_data=(x_val, y_val))\n",
    "\n",
    "scores = model.evaluate(x_test,y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
